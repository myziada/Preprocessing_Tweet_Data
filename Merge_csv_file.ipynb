{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Gabungkan banyak csv file menjadi 1 csv file"
      ],
      "metadata": {
        "id": "evWKi0xYP3mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CEK ROWS"
      ],
      "metadata": {
        "id": "KUrRHZKmcVRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def count_rows(csv_file):\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        row_count = sum(1 for row in reader)\n",
        "    return row_count\n",
        "\n",
        "# csv_file_path = '/content/val_Dongkol_Muak.csv'\n",
        "# total_rows = count_rows(csv_file_path)\n",
        "# print(\"Total rows_Dongkol_Muak:\", total_rows)\n",
        "\n",
        "# csv_file_path = '/content/val_Jengkel_Muak.csv'\n",
        "# total_rows = count_rows(csv_file_path)\n",
        "# print(\"Total rows_Jengkel_Muak:\", total_rows)\n",
        "\n",
        "# csv_file_path = '/content/val_Kesal_Muak.csv'\n",
        "# total_rows = count_rows(csv_file_path)\n",
        "# print(\"Total rows_Kesal_Muak:\", total_rows)\n",
        "\n",
        "# Usage example\n",
        "csv_file_path = '/content/val_Gembira_Gembira.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Gembira:\", total_rows)\n",
        "\n",
        "# Usage example\n",
        "csv_file_path = '/content/val_Marah_Gembira.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_MarahGembira:\", total_rows)\n",
        "\n",
        "# Usage example\n",
        "csv_file_path = '/content/val_Sedih_Gembira.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_SedihGembira:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Marah_Marah.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Marah:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Muak2_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Muak2:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Muak_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Muak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Kesal_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_KesalMuak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Jijik_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_JijikMuak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Benci_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_BenciMuak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Dongkol_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_DongkolMuak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Jengkel_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_JengkelMuak\", total_rows)\n",
        "\n",
        "# csv_file_path = '/content/val_Terkejut2_Muak.csv'\n",
        "# total_rows = count_rows(csv_file_path)\n",
        "# print(\"Total rows_Muak2:\", total_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGdWnhc3DYsR",
        "outputId": "9eea64c5-7126-4872-ac81-d5176f55e92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows_Gembira: 58\n",
            "Total rows_MarahGembira: 58\n",
            "Total rows_SedihGembira: 58\n",
            "Total rows_Marah: 203\n",
            "Total rows_Muak2: 72\n",
            "Total rows_Muak 8\n",
            "Total rows_KesalMuak 11\n",
            "Total rows_JijikMuak 30\n",
            "Total rows_BenciMuak 20\n",
            "Total rows_DongkolMuak 4\n",
            "Total rows_JengkelMuak 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def count_rows(csv_file):\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        row_count = sum(1 for row in reader)\n",
        "    return row_count\n",
        "\n",
        "csv_file_path = '/content/val_Sedih_Sedih.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Sedih_Sedih:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Sedih2_Sedih.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Sedih2_Sedih:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Gembira_Sedih.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Gembira_Sedih:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Marah_Sedih.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Marah_Sedih:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Muak_Sedih.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Muak_Sedih:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Takut_Sedih.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Takut_Sedih:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Marah3_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Marah3_Muak:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Gembira3_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Gembira3:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Takut3_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Takut3Muak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Sedih2_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Sedih2Muak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Terkejut2_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Terkejut2Muak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Marah_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_MarahMuak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Gembira_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_GembiraMuak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Takut_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_TakutMuak\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/_Muak2_Muak.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Muak2_Muak_Tanpa Val:\", total_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ako3Tm5XQfhH",
        "outputId": "950924ea-91ad-4f4a-e84f-ac36d4a026d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows_Sedih_Sedih: 101\n",
            "Total rows_Sedih2_Sedih: 29\n",
            "Total rows_Gembira_Sedih: 101\n",
            "Total rows_Marah_Sedih: 101\n",
            "Total rows_Muak_Sedih: 101\n",
            "Total rows_Takut_Sedih: 14\n",
            "Total rows_Marah3_Muak: 2\n",
            "Total rows_Gembira3: 1\n",
            "Total rows_Takut3Muak 1\n",
            "Total rows_Sedih2Muak 2\n",
            "Total rows_Terkejut2Muak 1\n",
            "Total rows_MarahMuak 8\n",
            "Total rows_GembiraMuak 8\n",
            "Total rows_TakutMuak 8\n",
            "Total rows_Muak2_Muak_Tanpa Val: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def count_rows(csv_file):\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        row_count = sum(1 for row in reader)\n",
        "    return row_count\n",
        "\n",
        "csv_file_path = '/content/val_Gembira_Takut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Gembira_Takut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Takut_Takut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Takut_Takut:\", total_rows)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYNSOT2DZ5pF",
        "outputId": "3063e15c-ba82-4aaf-8786-93f4628c78f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows_Gembira_Takut: 191\n",
            "Total rows_Takut_Takut: 191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def count_rows(csv_file):\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        row_count = sum(1 for row in reader)\n",
        "    return row_count\n",
        "\n",
        "csv_file_path = '/content/val_Gembira_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Gembira_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Marah_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Marah_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Takut_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Takut_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Terkejut_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Terkejut_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Muak_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Muak_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Muak2_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Muak2_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Sedih_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Sedih_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Sedih2_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Sedih2_Terkejut:\", total_rows)\n",
        "\n",
        "csv_file_path = '/content/val_Terkejut2_Terkejut.csv'\n",
        "total_rows = count_rows(csv_file_path)\n",
        "print(\"Total rows_Terkejut2_Terkejut:\", total_rows)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZqBYWScb0tF",
        "outputId": "6f0015fc-60f8-43cc-961b-c57c39b7b8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows_Gembira_Terkejut: 57\n",
            "Total rows_Marah_Terkejut: 57\n",
            "Total rows_Takut_Terkejut: 57\n",
            "Total rows_Terkejut_Terkejut: 57\n",
            "Total rows_Muak_Terkejut: 57\n",
            "Total rows_Muak2_Terkejut: 2\n",
            "Total rows_Sedih_Terkejut: 57\n",
            "Total rows_Sedih2_Terkejut: 3\n",
            "Total rows_Terkejut2_Terkejut: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the original CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/withLabel_val_Terkejut_AB_5000.csv.csv')\n",
        "\n",
        "# Extract the first 170 rows\n",
        "extracted_df = df.head(170)\n",
        "\n",
        "# Save the extracted DataFrame to a new CSV file\n",
        "extracted_df.to_csv('extracted_file_val_Terkejut_AB_5000.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ztqHbad6sVUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# List the file paths of the CSV files you want to merge\n",
        "file_paths = ['val_Gembira_5000DataNew.csv', 'val_Sedih_5000DataNew.csv','val_Marah_5000DataNew.csv', 'val_Muak5000DataNew.csv','val_Takut5000DataNew.csv', 'val_Terkejut5000DataNew.csv']\n",
        "\n",
        "# Create a list to store the merged data\n",
        "merged_data = []\n",
        "\n",
        "# Loop through each file path\n",
        "for file_path in file_paths:\n",
        "    # Read the CSV file\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Create a CSV reader object\n",
        "        reader = csv.reader(file)\n",
        "\n",
        "        # Skip the header in each file except the first one\n",
        "        if len(merged_data) > 0:\n",
        "            next(reader)\n",
        "\n",
        "        # Append the data to the merged_data list\n",
        "        merged_data.extend(reader)\n",
        "\n",
        "# Write the merged data to a new CSV file\n",
        "merged_file_path = 'merged_fix_5000.csv'\n",
        "with open(merged_file_path, 'w', newline='') as file:\n",
        "    # Create a CSV writer object\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writerow(merged_data[0])\n",
        "\n",
        "    # Write the data rows\n",
        "    writer.writerows(merged_data[1:])\n",
        "\n",
        "print(f\"Merged file saved as {merged_file_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VheiM-OQr0lc",
        "outputId": "96bfe4f5-b2f7-4af9-8160-6753fd69b028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged file saved as merged_fix_5000.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data from the first CSV file\n",
        "data1 = pd.read_csv('training data.csv')\n",
        "\n",
        "# Load data from the second CSV file\n",
        "data2 = pd.read_csv('merged (1) (1).csv')\n",
        "\n",
        "# Find the data that is present in data1 but not in data2\n",
        "data_not_in_data2 = data1[~data1.isin(data2)].dropna()\n",
        "\n",
        "# Save the data that is not in data2 to a new CSV file\n",
        "data_not_in_data2.to_csv('data_not_in_data2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Vp9gMRl0NfAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/extracted_file_TextLabel_1021_Data.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "69HrSrw5w2OE",
        "outputId": "fd6917c5-fd85-44e1-815c-af75577836c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text     label\n",
              "0     kocak dukung anis mah wajah seringai keringat ...   gembira\n",
              "1     blash kelas coy guling guling lantai tertawa w...   gembira\n",
              "2     orang munafik nya cepat lupa prilakunya engak ...   gembira\n",
              "3     asli bego bego bang circle anak muda engak but...   gembira\n",
              "4                                rekam jejak ayat mayat   gembira\n",
              "...                                                 ...       ...\n",
              "1015  safari politik sindir adu prestasi ganjar pran...  terkejut\n",
              "1016  budaya cipta orang intelektualitas moral menta...  terkejut\n",
              "1017  moga lancar berkah kunjunganya anies sangan be...  terkejut\n",
              "1018  oke tangan jempol wajah seringai wajah keringa...  terkejut\n",
              "1019  tanding lari gandjar jalan masauk got hahahaha...  terkejut\n",
              "\n",
              "[1020 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-700741ff-64e0-43c6-a780-31169e13a5ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kocak dukung anis mah wajah seringai keringat ...</td>\n",
              "      <td>gembira</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>blash kelas coy guling guling lantai tertawa w...</td>\n",
              "      <td>gembira</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>orang munafik nya cepat lupa prilakunya engak ...</td>\n",
              "      <td>gembira</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asli bego bego bang circle anak muda engak but...</td>\n",
              "      <td>gembira</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rekam jejak ayat mayat</td>\n",
              "      <td>gembira</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>safari politik sindir adu prestasi ganjar pran...</td>\n",
              "      <td>terkejut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>budaya cipta orang intelektualitas moral menta...</td>\n",
              "      <td>terkejut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>moga lancar berkah kunjunganya anies sangan be...</td>\n",
              "      <td>terkejut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>oke tangan jempol wajah seringai wajah keringa...</td>\n",
              "      <td>terkejut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>tanding lari gandjar jalan masauk got hahahaha...</td>\n",
              "      <td>terkejut</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1020 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-700741ff-64e0-43c6-a780-31169e13a5ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-700741ff-64e0-43c6-a780-31169e13a5ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-700741ff-64e0-43c6-a780-31169e13a5ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the original CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/output_stopword_lemmatization_1021_Data.csv')\n",
        "# 'datetime', 'teks', 'text', 'numtokens', 'numlextokens', 'avglexval', 'lexratio', 'label'\n",
        "# Specify the columns you want to extract\n",
        "columns_to_extract = ['text', 'label']  # Replace with your desired column names\n",
        "\n",
        "# Create a new DataFrame with only the specified columns\n",
        "extracted_df = df[columns_to_extract]\n",
        "\n",
        "# Save the extracted DataFrame to a new CSV file\n",
        "extracted_df.to_csv('extracted_file_TextLabel_1021_Data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "fW_KytSpxOCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file and extract the header\n",
        "df = pd.read_csv('/content/output_stopword_lemmatization_1021_Data.csv', header=0)\n",
        "\n",
        "# Get the column names\n",
        "column_names = df.columns.tolist()\n",
        "\n",
        "# Print the column names\n",
        "print(\"Column names:\", column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJluESQ6xXD5",
        "outputId": "09fe2e34-f728-40bc-dc22-b5e6ce0ff98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names: ['datetime', 'teks', 'text', 'numtokens', 'numlextokens', 'avglexval', 'lexratio', 'label']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def merge_csv_files(input_folder, output_file):\n",
        "    csv_files = [file for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
        "\n",
        "    with open(output_file, 'w', newline='') as output_csv:\n",
        "        writer = csv.writer(output_csv)\n",
        "\n",
        "        for file in csv_files:\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            with open(file_path, 'r') as input_csv:\n",
        "                reader = csv.reader(input_csv)\n",
        "                for row in reader:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "# Example usage\n",
        "input_folder = '/content/dataset'  # Specify the folder path where the CSV files are located\n",
        "output_file = 'merged_val_All_tambahan_Muak.csv'  # Specify the output file name\n",
        "\n",
        "merge_csv_files(input_folder, output_file)\n"
      ],
      "metadata": {
        "id": "49WCAozsDSMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# file yg sudah di translate\n",
        "df = pd.read_csv('/content/merged_val_All_tambahan_Muak.csv')\n",
        "print(df)\n",
        "import pandas as pd\n",
        "# file yg sudah di translate\n",
        "df1 = pd.read_csv('/content/merged_val_All_tambahan_Muak1.csv')\n",
        "print(df1)"
      ],
      "metadata": {
        "id": "6VVuDJ3oNCVQ",
        "outputId": "9f07d5c0-e65c-40ef-9c58-b1596f3e39b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     datetime  \\\n",
            "0   2023-05-14 14:52:40+00:00   \n",
            "1   2022-10-19 09:06:59+00:00   \n",
            "2   2019-08-01 01:48:13+00:00   \n",
            "3                    datetime   \n",
            "4   2017-10-16 22:45:33+00:00   \n",
            "5   2017-09-03 06:05:01+00:00   \n",
            "6   2017-09-03 05:43:09+00:00   \n",
            "7   2017-09-03 05:34:11+00:00   \n",
            "8   2017-04-29 04:58:17+00:00   \n",
            "9   2017-04-20 04:31:20+00:00   \n",
            "10  2017-03-08 01:41:15+00:00   \n",
            "11  2017-02-24 03:26:20+00:00   \n",
            "12  2017-02-08 04:17:01+00:00   \n",
            "13  2017-01-05 11:18:40+00:00   \n",
            "14  2017-01-05 11:06:14+00:00   \n",
            "15  2016-11-26 00:22:16+00:00   \n",
            "16  2016-11-25 15:10:39+00:00   \n",
            "17  2016-10-13 05:17:27+00:00   \n",
            "18  2016-06-15 19:01:28+00:00   \n",
            "19  2015-12-19 20:50:21+00:00   \n",
            "20  2014-07-22 09:06:53+00:00   \n",
            "21  2014-05-30 09:33:04+00:00   \n",
            "22  2011-08-22 15:03:59+00:00   \n",
            "23                   datetime   \n",
            "24  2023-02-02 17:35:01+00:00   \n",
            "25  2022-04-03 15:30:54+00:00   \n",
            "26  2021-11-03 09:56:27+00:00   \n",
            "27  2021-01-26 06:32:01+00:00   \n",
            "\n",
            "                                              content  \\\n",
            "0   @Danielmunaf6 @tvonenews hahaha :grinning_face...   \n",
            "1   @Aminkank @santorinissun gubernur dongkol anie...   \n",
            "2   Dongkol !!Anies Baswedan Dan Tri Rismaharini.\\...   \n",
            "3                                             content   \n",
            "4   Komunitas tionghoa di jakarta marah atas ucapa...   \n",
            "5   Anies Baswedan Marah Besar Dan Inilah Seruan S...   \n",
            "6   Anies Baswedan Marah Besar Dan Inilah Seruan S...   \n",
            "7   Anies Baswedan Marah Besar Dan Inilah Seruan S...   \n",
            "8   Ahok Tertibkan Hunian Liar Yang Bikin Kumuh Ja...   \n",
            "9   Saya Mendengar Kisa Pak Anies Baswedan Yg Dibe...   \n",
            "10  Anies Baswedan Ngeles Dan Marah Lalu Menyalahk...   \n",
            "11  Kabar Pagi ini Geger ,, Anies Baswedan Marah B...   \n",
            "12  Semua Kata-Kata Tersebut Diucapkan Anies Baswe...   \n",
            "13  Pendukung Ahok Ini Marah Dan Sebut Pandangan N...   \n",
            "14  Pendukung Ahok ini Marah Dan Sebut Pandangan N...   \n",
            "15  Anies Baswedan: Saya Tahat Bisa Terima Dan Mar...   \n",
            "16  Anies Baswedan: Saya Tahat Bisa Terima Dan Mar...   \n",
            "17  @Breengoss Itu karena anies baswedan Amat muna...   \n",
            "18  Ketika Menteri Anis Marah Kepada Anak Blahnya\\...   \n",
            "19  Baru Ngeh, aa ini marah banget pada presiden D...   \n",
            "20  @agus_noor @bluesmerbabu Kata mas anies Baswed...   \n",
            "21  Beda Pilihan Figur It Normal, Wajar & amp; Man...   \n",
            "22  \"Kita Tahu+Marah Saat Alam Dijarah Asing, Tapi...   \n",
            "23                                            content   \n",
            "24  KASIHAN GANGGAN KUBU LADA, Rocky Gerung Jengke...   \n",
            "25  Haloo @antontenabang02 :grinning_face_with_big...   \n",
            "26  @Aagym Benar A Kebanyakan Pembenci Jokowi Begi...   \n",
            "27  HABIBUROKHMAN MARAH DENGAN KADER GERINDRA YANG...   \n",
            "\n",
            "                                                 text  numTokens  \\\n",
            "0   hahaha grinning face with smiling eyes lawan k...         30   \n",
            "1   gubernur dongkol anies baswedan rampas merdeka...         13   \n",
            "2   dongkol anies baswedan tri rismaharini gubernu...         17   \n",
            "3                                                text  numTokens   \n",
            "4   komunitas tionghoa jakarta marah ucap anies ba...         11   \n",
            "5   anies baswedan marah seru tuntut gubernur dki ...          8   \n",
            "6   anies baswedan marah seru tuntut gubernur dki ...         12   \n",
            "7   anies baswedan marah seru tuntut gubernur dki ...         12   \n",
            "8   ahok tertib huni liar bikin kumuh jakarta anie...         13   \n",
            "9   dengar kisa anies baswedan henti kursi menteri...          9   \n",
            "10  anies baswedan kilah marah salah kompas tv dat...          9   \n",
            "11  kabar pagi geger anies baswedan marah sandiaga...         11   \n",
            "12  katakata anies baswedan dang mimik wajah marah...         11   \n",
            "13  dukung ahok marah pandang negarawan intelektua...          9   \n",
            "14  dukung ahok marah pandang negarawan intelektua...          8   \n",
            "15  anies baswedan tahat terima marah keras negara...          8   \n",
            "16    anies baswedan tahat terima marah keras myanmar          7   \n",
            "17  anies baswedan munafik curu ugm banya marah ke...          9   \n",
            "18  menteri anis marah anak blahnya catat tulis me...         12   \n",
            "19  sadar aa marah banget presiden anies baswedan ...         11   \n",
            "20    mas anies baswedan layar jkw marah percaya wowo          8   \n",
            "21  beda pilih figur normal wajar amp manusiawisan...         12   \n",
            "22  tahumarah alam jarah asing tahu marah rakyat t...          9   \n",
            "23                                               text  numTokens   \n",
            "24  kasihan ganggan kubu lada rocky gerung jengkel...         16   \n",
            "25  haloo grinning face with big eyeswaving hand s...         24   \n",
            "26  a banyak benci jokowi emosional muda sulut mud...         20   \n",
            "27  habiburokhman marah kader gerindra mista anies...         28   \n",
            "\n",
            "    numLexTokens  avgLexVal            lexRatio   Label1  Label2  \n",
            "0              1        1.0  0.0333333333333333  Dongkol    Muak  \n",
            "1              1        1.0  0.0769230769230769  Dongkol    Muak  \n",
            "2              1        1.0  0.0588235294117647  Dongkol    Muak  \n",
            "3   numLexTokens  avgLexVal            lexRatio   Label1  Label2  \n",
            "4              1        1.0  0.0909090909090909    Benci    Muak  \n",
            "5              2        1.0                0.25    Benci    Muak  \n",
            "6              2        1.0  0.1666666666666666    Benci    Muak  \n",
            "7              3        1.0                0.25    Benci    Muak  \n",
            "8              2        1.0  0.1538461538461538    Benci    Muak  \n",
            "9              1        1.0  0.1111111111111111    Benci    Muak  \n",
            "10             2        1.0  0.2222222222222222    Benci    Muak  \n",
            "11             2        1.0  0.1818181818181818    Benci    Muak  \n",
            "12             2        1.0  0.1818181818181818    Benci    Muak  \n",
            "13             1        1.0  0.1111111111111111    Benci    Muak  \n",
            "14             1        1.0               0.125    Benci    Muak  \n",
            "15             1        1.0               0.125    Benci    Muak  \n",
            "16             1        1.0  0.1428571428571428    Benci    Muak  \n",
            "17             2        1.0  0.2222222222222222    Benci    Muak  \n",
            "18             1        1.0  0.0833333333333333    Benci    Muak  \n",
            "19             1        1.0  0.0909090909090909    Benci    Muak  \n",
            "20             1        1.0               0.125    Benci    Muak  \n",
            "21             2        1.0  0.1666666666666666    Benci    Muak  \n",
            "22             2        1.0  0.2222222222222222    Benci    Muak  \n",
            "23  numLexTokens  avgLexVal            lexRatio   Label1  Label2  \n",
            "24             1        1.0              0.0625  Jengkel    Muak  \n",
            "25             1        1.0  0.0416666666666666  Jengkel    Muak  \n",
            "26             3        1.0                0.15  Jengkel    Muak  \n",
            "27             2        1.0  0.0714285714285714  Jengkel    Muak  \n",
            "                                             datetime  \\\n",
            "0   2023-05-14 14:52:40+00:00,\"@Danielmunaf6 @tvon...   \n",
            "1                    Lawan Semakin Kesel Dan Dongkol;   \n",
            "2            Terbukti anies Baswedan Semakin Terdepan   \n",
            "3   Salam Perubahan :raised_hand: https://t.co/o1i...   \n",
            "4   2022-10-19 09:06:59+00:00,\"@Aminkank @santorin...   \n",
            "5   2019-08-01 01:48:13+00:00,\"Dongkol !!Anies Bas...   \n",
            "6               Gubernur Dki Jakarta anies Mengatakan   \n",
            "7                                                   ;   \n",
            "8                            https://t.co/etfrga20td;   \n",
            "9   @el_francs @endiwidjajak @fathir97 @mcfatimah ...   \n",
            "10                          2017-10-16 22:45:33+00:00   \n",
            "11                          2017-09-03 06:05:01+00:00   \n",
            "12                          2017-09-03 05:43:09+00:00   \n",
            "13                          2017-09-03 05:34:11+00:00   \n",
            "14                          2017-04-29 04:58:17+00:00   \n",
            "15                          2017-04-20 04:31:20+00:00   \n",
            "16                          2017-03-08 01:41:15+00:00   \n",
            "17  2017-02-24 03:26:20+00:00,\"Kabar Pagi ini Gege...   \n",
            "18                          2017-02-08 04:17:01+00:00   \n",
            "19                          2017-01-05 11:18:40+00:00   \n",
            "20                          2017-01-05 11:06:14+00:00   \n",
            "21                          2016-11-26 00:22:16+00:00   \n",
            "22                          2016-11-25 15:10:39+00:00   \n",
            "23  2016-10-13 05:17:27+00:00,\"@Breengoss Itu kare...   \n",
            "24  2016-06-15 19:01:28+00:00,\"Ketika Menteri Anis...   \n",
            "25                                          Catatan:;   \n",
            "26  Tulisan Menteri Pendidikan Dan Kebudayaan Anie...   \n",
            "27  2015-12-19 20:50:21+00:00,\"Baru Ngeh, aa ini m...   \n",
            "28                          2014-07-22 09:06:53+00:00   \n",
            "29  2014-05-30 09:33:04+00:00,\"Beda Pilihan Figur ...   \n",
            "30  2011-08-22 15:03:59+00:00,\"\"\"Kita Tahu+Marah S...   \n",
            "31  2023-02-02 17:35:01+00:00,\"KASIHAN GANGGAN KUB...   \n",
            "32  2022-04-03 15:30:54+00:00,\"Haloo @antontenaban...   \n",
            "33                                         ~ Thread.;   \n",
            "34                                       Pilpres 2024   \n",
            "35                                                  ;   \n",
            "36                                 Oleh Asyari Usman;   \n",
            "37                                                  ;   \n",
            "38  Lelah Dan Jenuh.Bercampur Marah Bua.Rata-Rata ...   \n",
            "39  2021-11-03 09:56:27+00:00,\"@Aagym Benar A Keba...   \n",
            "40                                                  ;   \n",
            "41  Padahal jelas anies gk kerja hanya kata dan ka...   \n",
            "42  2021-01-26 06:32:01+00:00,\"HABIBUROKHMAN MARAH...   \n",
            "\n",
            "                                              content  \\\n",
            "0                                                 NaN   \n",
            "1                                                 NaN   \n",
            "2                                    Dimanapa Singgah   \n",
            "3                                                 NaN   \n",
            "4                                                 NaN   \n",
            "5                                                 NaN   \n",
            "6    Persoalan Sampah di Jakarta Akan Diurus Oleh ...   \n",
            "7                                                 NaN   \n",
            "8                                                 NaN   \n",
            "9                                                 NaN   \n",
            "10  Komunitas tionghoa di jakarta marah atas ucapa...   \n",
            "11  Anies Baswedan Marah Besar Dan Inilah Seruan S...   \n",
            "12  Anies Baswedan Marah Besar Dan Inilah Seruan S...   \n",
            "13  Anies Baswedan Marah Besar Dan Inilah Seruan S...   \n",
            "14  Ahok Tertibkan Hunian Liar Yang Bikin Kumuh Ja...   \n",
            "15  Saya Mendengar Kisa Pak Anies Baswedan Yg Dibe...   \n",
            "16  Anies Baswedan Ngeles Dan Marah Lalu Menyalahk...   \n",
            "17                                                NaN   \n",
            "18  Semua Kata-Kata Tersebut Diucapkan Anies Baswe...   \n",
            "19  Pendukung Ahok Ini Marah Dan Sebut Pandangan N...   \n",
            "20  Pendukung Ahok ini Marah Dan Sebut Pandangan N...   \n",
            "21  Anies Baswedan: Saya Tahat Bisa Terima Dan Mar...   \n",
            "22  Anies Baswedan: Saya Tahat Bisa Terima Dan Mar...   \n",
            "23                                                NaN   \n",
            "24                                                NaN   \n",
            "25                                                NaN   \n",
            "26                                                NaN   \n",
            "27                                                NaN   \n",
            "28  @agus_noor @bluesmerbabu Kata mas anies Baswed...   \n",
            "29   Tidak Perlu Kecewa Dan Marah Pada Orang Lain ...   \n",
            "30                                                NaN   \n",
            "31                                                NaN   \n",
            "32                                                NaN   \n",
            "33                                                NaN   \n",
            "34      Titik Balik Indonesia Bersama Anies Baswedan;   \n",
            "35                                                NaN   \n",
            "36                                                NaN   \n",
            "37                                                NaN   \n",
            "38                                                NaN   \n",
            "39                                                NaN   \n",
            "40                                                NaN   \n",
            "41                                                NaN   \n",
            "42                                                NaN   \n",
            "\n",
            "                                                 text  numTokens  \\\n",
            "0                                                 NaN        NaN   \n",
            "1                                                 NaN        NaN   \n",
            "2    rakyat tumpah ruwah Menyambut Sang Perubahan....        NaN   \n",
            "3                                                 NaN        NaN   \n",
            "4                                                 NaN        NaN   \n",
            "5                                                 NaN        NaN   \n",
            "6                                                 NaN        NaN   \n",
            "7                                                 NaN        NaN   \n",
            "8                                                 NaN        NaN   \n",
            "9                                                 NaN        NaN   \n",
            "10  komunitas tionghoa jakarta marah ucap anies ba...       11.0   \n",
            "11  anies baswedan marah seru tuntut gubernur dki ...        8.0   \n",
            "12  anies baswedan marah seru tuntut gubernur dki ...       12.0   \n",
            "13  anies baswedan marah seru tuntut gubernur dki ...       12.0   \n",
            "14  ahok tertib huni liar bikin kumuh jakarta anie...       13.0   \n",
            "15  dengar kisa anies baswedan henti kursi menteri...        9.0   \n",
            "16  anies baswedan kilah marah salah kompas tv dat...        9.0   \n",
            "17                                                NaN        NaN   \n",
            "18  katakata anies baswedan dang mimik wajah marah...       11.0   \n",
            "19  dukung ahok marah pandang negarawan intelektua...        9.0   \n",
            "20  dukung ahok marah pandang negarawan intelektua...        8.0   \n",
            "21  anies baswedan tahat terima marah keras negara...        8.0   \n",
            "22    anies baswedan tahat terima marah keras myanmar        7.0   \n",
            "23                                                NaN        NaN   \n",
            "24                                                NaN        NaN   \n",
            "25                                                NaN        NaN   \n",
            "26                                                NaN        NaN   \n",
            "27                                                NaN        NaN   \n",
            "28    mas anies baswedan layar jkw marah percaya wowo        8.0   \n",
            "29  beda pilih figur normal wajar amp manusiawisan...       12.0   \n",
            "30                                                NaN        NaN   \n",
            "31                                                NaN        NaN   \n",
            "32                                                NaN        NaN   \n",
            "33                                                NaN        NaN   \n",
            "34                                                NaN        NaN   \n",
            "35                                                NaN        NaN   \n",
            "36                                                NaN        NaN   \n",
            "37                                                NaN        NaN   \n",
            "38                                                NaN        NaN   \n",
            "39                                                NaN        NaN   \n",
            "40                                                NaN        NaN   \n",
            "41                                                NaN        NaN   \n",
            "42                                                NaN        NaN   \n",
            "\n",
            "    numLexTokens  avgLexVal  lexRatio Label1 Label2;  \n",
            "0            NaN        NaN       NaN    NaN     NaN  \n",
            "1            NaN        NaN       NaN    NaN     NaN  \n",
            "2            NaN        NaN       NaN    NaN     NaN  \n",
            "3            NaN        NaN       NaN    NaN     NaN  \n",
            "4            NaN        NaN       NaN    NaN     NaN  \n",
            "5            NaN        NaN       NaN    NaN     NaN  \n",
            "6            NaN        NaN       NaN    NaN     NaN  \n",
            "7            NaN        NaN       NaN    NaN     NaN  \n",
            "8            NaN        NaN       NaN    NaN     NaN  \n",
            "9            NaN        NaN       NaN    NaN     NaN  \n",
            "10           1.0        1.0  0.090909  Benci   Muak;  \n",
            "11           2.0        1.0  0.250000  Benci   Muak;  \n",
            "12           2.0        1.0  0.166667  Benci   Muak;  \n",
            "13           3.0        1.0  0.250000  Benci   Muak;  \n",
            "14           2.0        1.0  0.153846  Benci   Muak;  \n",
            "15           1.0        1.0  0.111111  Benci   Muak;  \n",
            "16           2.0        1.0  0.222222  Benci   Muak;  \n",
            "17           NaN        NaN       NaN    NaN     NaN  \n",
            "18           2.0        1.0  0.181818  Benci   Muak;  \n",
            "19           1.0        1.0  0.111111  Benci   Muak;  \n",
            "20           1.0        1.0  0.125000  Benci   Muak;  \n",
            "21           1.0        1.0  0.125000  Benci   Muak;  \n",
            "22           1.0        1.0  0.142857  Benci   Muak;  \n",
            "23           NaN        NaN       NaN    NaN     NaN  \n",
            "24           NaN        NaN       NaN    NaN     NaN  \n",
            "25           NaN        NaN       NaN    NaN     NaN  \n",
            "26           NaN        NaN       NaN    NaN     NaN  \n",
            "27           NaN        NaN       NaN    NaN     NaN  \n",
            "28           1.0        1.0  0.125000  Benci   Muak;  \n",
            "29           2.0        1.0  0.166667  Benci   Muak\"  \n",
            "30           NaN        NaN       NaN    NaN     NaN  \n",
            "31           NaN        NaN       NaN    NaN     NaN  \n",
            "32           NaN        NaN       NaN    NaN     NaN  \n",
            "33           NaN        NaN       NaN    NaN     NaN  \n",
            "34           NaN        NaN       NaN    NaN     NaN  \n",
            "35           NaN        NaN       NaN    NaN     NaN  \n",
            "36           NaN        NaN       NaN    NaN     NaN  \n",
            "37           NaN        NaN       NaN    NaN     NaN  \n",
            "38           NaN        NaN       NaN    NaN     NaN  \n",
            "39           NaN        NaN       NaN    NaN     NaN  \n",
            "40           NaN        NaN       NaN    NaN     NaN  \n",
            "41           NaN        NaN       NaN    NaN     NaN  \n",
            "42           NaN        NaN       NaN    NaN     NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# file yg sudah di translate\n",
        "df = pd.read_csv('/content/extracted_file_TextLabel_1021_Data.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7bPXgn2v-Un",
        "outputId": "13ac30f8-467a-41fb-8e7d-0a13a0ca0201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text     label\n",
            "0     kocak dukung anis mah wajah seringai keringat ...   gembira\n",
            "1     blash kelas coy guling guling lantai tertawa w...   gembira\n",
            "2     orang munafik nya cepat lupa prilakunya engak ...   gembira\n",
            "3     asli bego bego bang circle anak muda engak but...   gembira\n",
            "4                                rekam jejak ayat mayat   gembira\n",
            "...                                                 ...       ...\n",
            "1015  safari politik sindir adu prestasi ganjar pran...  terkejut\n",
            "1016  budaya cipta orang intelektualitas moral menta...  terkejut\n",
            "1017  moga lancar berkah kunjunganya anies sangan be...  terkejut\n",
            "1018  oke tangan jempol wajah seringai wajah keringa...  terkejut\n",
            "1019  tanding lari gandjar jalan masauk got hahahaha...  terkejut\n",
            "\n",
            "[1020 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def merge_csv_files(input_folder, output_file):\n",
        "    csv_files = [file for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
        "\n",
        "    with open(output_file, 'w', newline='') as output_csv:\n",
        "        writer = csv.writer(output_csv)\n",
        "\n",
        "        for file in csv_files:\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            with open(file_path, 'r') as input_csv:\n",
        "                reader = csv.reader(input_csv)\n",
        "                for row in reader:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "# Example usage\n",
        "input_folder = '/content/dataset_Takut'  # Specify the folder path where the CSV files are located\n",
        "output_file = 'merged_val_All_Takut_Additional.csv'  # Specify the output file name\n",
        "\n",
        "merge_csv_files(input_folder, output_file)\n"
      ],
      "metadata": {
        "id": "YGPzpHBAHq5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "directory = '/content/dataset'\n",
        "output_file = 'output.csv'\n",
        "\n",
        "csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
        "\n",
        "with open(output_file, 'w', newline='') as outfile:\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    for file in csv_files:\n",
        "        with open(file, 'r') as infile:\n",
        "            reader = csv.reader(infile)\n",
        "            next(reader)  # Skip the header row\n",
        "\n",
        "            for row in reader:\n",
        "                writer.writerow(row)\n",
        "\n",
        "print('Merging CSV files complete!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcDuBfmPemLX",
        "outputId": "b7ac1277-f779-4dd6-cd6a-b25e2fcf4000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging CSV files complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def merge_csv_files(input_folder, output_file):\n",
        "    csv_files = [file for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
        "\n",
        "    with open(output_file, 'w', newline='') as output_csv:\n",
        "        writer = csv.writer(output_csv)\n",
        "\n",
        "        for file in csv_files:\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            with open(file_path, 'r') as input_csv:\n",
        "                reader = csv.reader(input_csv)\n",
        "                for row in reader:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "# Example usage\n",
        "input_folder = '/content/dataset_Sedih'  # Specify the folder path where the CSV files are located\n",
        "output_file = 'merged_val_All_Sedih_New.csv'  # Specify the output file name\n",
        "\n",
        "merge_csv_files(input_folder, output_file)\n"
      ],
      "metadata": {
        "id": "nccmwCKIHuxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def merge_csv_files(input_folder, output_file):\n",
        "    csv_files = [file for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
        "\n",
        "    with open(output_file, 'w', newline='') as output_csv:\n",
        "        writer = csv.writer(output_csv)\n",
        "\n",
        "        for file in csv_files:\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            with open(file_path, 'r') as input_csv:\n",
        "                reader = csv.reader(input_csv)\n",
        "                for row in reader:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "# Example usage\n",
        "input_folder = '//content/dataset_Terkejut'  # Specify the folder path where the CSV files are located\n",
        "output_file = 'merged_val_All_Terkejut_New.csv'  # Specify the output file name\n",
        "\n",
        "merge_csv_files(input_folder, output_file)\n"
      ],
      "metadata": {
        "id": "mmxJpUaIHxsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def merge_csv_files(input_folder, output_file):\n",
        "    csv_files = [file for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
        "\n",
        "    with open(output_file, 'w', newline='') as output_csv:\n",
        "        writer = csv.writer(output_csv)\n",
        "\n",
        "        for file in csv_files:\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            with open(file_path, 'r') as input_csv:\n",
        "                reader = csv.reader(input_csv)\n",
        "                for row in reader:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "# Example usage\n",
        "input_folder = '/content/dataset'  # Specify the folder path where the CSV files are located\n",
        "output_file = 'merged_val_Gembira.csv'  # Specify the output file name\n",
        "\n",
        "merge_csv_files(input_folder, output_file)\n"
      ],
      "metadata": {
        "id": "sbZ6alGbR6U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file"
      ],
      "metadata": {
        "id": "zsMeXt-q1gXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NRCLex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz_o1UHa0gUH",
        "outputId": "858910ac-dcee-454d-b5bf-31a884dc29cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting NRCLex\n",
            "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from NRCLex) (0.17.1)\n",
            "INFO: pip is looking at multiple versions of nrclex to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob->NRCLex) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (4.65.0)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43308 sha256=8601a29490daf47faee4c1492654cf5f3410863c336270371f3c8c612b18bafc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/10/44/6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: NRCLex\n",
            "Successfully installed NRCLex-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacytextblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "RUlK5yjn1XwQ",
        "outputId": "0d453033-4b0e-403d-b8da-9004c9fb9aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacytextblob\n",
            "  Downloading spacytextblob-4.0.0-py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: spacy<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from spacytextblob) (3.5.2)\n",
            "Collecting textblob<0.16.0,>=0.15.3 (from spacytextblob)\n",
            "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.5/636.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.3.0)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob<0.16.0,>=0.15.3->spacytextblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0,>=3.0->spacytextblob) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0,>=3.0->spacytextblob) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0,>=3.0->spacytextblob) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0,>=3.0->spacytextblob) (2.1.2)\n",
            "Installing collected packages: textblob, spacytextblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.17.1\n",
            "    Uninstalling textblob-0.17.1:\n",
            "      Successfully uninstalled textblob-0.17.1\n",
            "Successfully installed spacytextblob-4.0.0 textblob-0.15.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "textblob"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "# Import required modules\n",
        "from nrclex import NRCLex\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download specific corpora\n",
        "TextBlob.download_corpora()\n",
        "\n",
        "# Alternatively, you can specify the corpora to download\n",
        "# TextBlob.download_corpora(\"corpora_name\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "k1TRULBH01jO",
        "outputId": "5b4fe8bb-4640-43dc-8104-9e67f0c2cf74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ac05b863dffb>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Download specific corpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mTextBlob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_corpora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Alternatively, you can specify the corpora to download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'TextBlob' has no attribute 'download_corpora'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning list of words\n",
        "text = ['hate', 'lovely', 'person', 'worst']\n"
      ],
      "metadata": {
        "id": "BN_nggHw012F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import module\n",
        "from nrclex import NRCLex\n",
        "\n",
        "# Assign list of strings\n",
        "text = ['hate', 'lovely', 'person', 'worst']\n",
        "\n",
        "# Iterate through list\n",
        "for i in range(len(text)):\n",
        "\n",
        "\t# Create object\n",
        "\temotion = NRCLex(text[i])\n",
        "\n",
        "\t# Classify emotion\n",
        "\tprint('\\n\\n', text[i], ': ', emotion.top_emotions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FI5jv1qM0_2Q",
        "outputId": "8ac40c89-5606-4cdf-c0de-029b1e41fd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - ''\n",
            "**********************************************************************\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MissingCorpusError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textblob/tokenizers.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m'''Return a list of sentences.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-35dfc1c330f6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Create object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0memotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNRCLex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Classify emotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nrclex.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   2871\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2872\u001b[0m         \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2873\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2874\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2875\u001b[0m         \u001b[0mbuild_word_affect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mWordList\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mWordList\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mword\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \"\"\"\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWordList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_punc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textblob/tokenizers.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, include_punc, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         _word_tokenizer.itokenize(sentence, include_punc=include_punc,\n\u001b[1;32m     72\u001b[0m                                 *args, **kwargs)\n\u001b[0;32m---> 73\u001b[0;31m         for sentence in sent_tokenize(text))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textblob/base.py\u001b[0m in \u001b[0;36mitokenize\u001b[0;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m##### SENTIMENT ANALYZERS ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textblob/decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingCorpusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pqbei7I0_5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxTQVORN016x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}